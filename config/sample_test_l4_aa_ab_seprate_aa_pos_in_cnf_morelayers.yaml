data:
  data: cath3_rescale_30_helix_separate2
  train_path: 'data/train_helix_data_encoded_using_coords_dec_2023_for_flow_matching_all.pkl'
  test_path: 'data/test_helix_data_encoded_using_coords_dec_2023_for_flow_matching_all.pkl'

  min_length: 8
  max_length: 30
  scale_coords: 0.0333

model:
  irreps_in:
  irreps_node_embedding: 128x0e+64x1e+32x2e
  num_layers: 8 #previous is 4
  irreps_node_attr: 1x0e
  irreps_sh: 1x0e+1x1e+1x2e
  max_radius: 8.0
  number_of_basis: 64
  fc_neurons: [64, 64]
  irreps_feature: 128x0e+64x1e+32x2e
  irreps_head: 32x0e+16x1e+8x2e
  num_heads: 4
  irreps_pre_attn:
  rescale_degree: False
  nonlinear_message: True
  irreps_mlp_mid: 384x0e+192x1e+96x2e
  norm_layer: layer
  alpha_drop: 0.2
  proj_drop: 0.0
  out_drop: 0.0
  drop_path_rate: 0.0
  mean:
  std:
  scale:
  atomref:
  bb_only: False
  self_cond: False
  type: unconditional



train:
  name: cath3_rescale_30_ab_separate2
  num_epochs: 2000
  batch_size: 16
  grad_accum: 4 # effective batch size = batch_size * grad_accumff
  save_interval: 2  # previous is 100. I plan to change the code
  print_interval: 1
  sample_interval: 4 # preiouvs is 100. 
  reduce_mean: False 
  lr: 2.0e-4
  lr_schedule: False
  ema: 0.999
  weight_decay: 1.0e-12
  grad_norm: 1.0
  lr_decay: 0.999
  eps: 2.0e-4 #1.0e-3
  optimizer: AdamW

ckpt: '61' 

sample:
  batch_size: 1
  n_samples: 64
